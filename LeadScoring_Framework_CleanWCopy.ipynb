{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leads Scoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Import Statements --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas_profiling\\plot.py:15: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-9aaa1565970d>\", line 34, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-108>\", line 2, in matplotlib\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(BACKEND)\n"
     ]
    }
   ],
   "source": [
    "# All Imports\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)\n",
    "\n",
    "############### Write code below ##################\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pymysql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas_profiling \n",
    "\n",
    "import locale\n",
    "from locale import atof\n",
    "locale.setlocale(locale.LC_NUMERIC, '')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## All import statements here --\n",
    "\n",
    "def disp_missing_val(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})\n",
    "    missing_value_df.plot(kind='bar')\n",
    "    \n",
    "def pct2float(x):\n",
    "    return float(x.strip('%'))/100\n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'    \n",
    "    \n",
    "def mysql_db_conn():\n",
    "    host = \"i2rds-read-replica2.cuoivis0lb68.us-east-1.rds.amazonaws.com\"\n",
    "    port = 3306\n",
    "    user = \"product\"\n",
    "    password = \"pr0ducT3\"\n",
    "    database = \"melv1n_crm2\" \n",
    "    \n",
    "    conn = pymysql.connect(\n",
    "    host=host,\n",
    "    port=int(port),\n",
    "    user=user,\n",
    "    passwd=password,\n",
    "    db=database,\n",
    "    charset='utf8mb4')\n",
    "    \n",
    "    return conn  \n",
    "\n",
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = pandas_profiling.ProfileReport(df)\n",
    "    profile.to_file(outputfile = filename)\n",
    "    print(\"Data profiling done\")\n",
    "    \n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "def optimize_df_mem(df):\n",
    "    # select only categorical fields\n",
    "    gl_obj = df.select_dtypes(include=['object']).copy()    \n",
    "    converted_obj = pd.DataFrame()\n",
    "    \n",
    "    for col in gl_obj.columns:\n",
    "        num_unique_values = len(gl_obj[col].unique())\n",
    "        num_total_values = len(gl_obj[col])\n",
    "        converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
    "        \"\"\"if num_unique_values / num_total_values < 0.5:\n",
    "            converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
    "        else:\n",
    "            converted_obj.loc[:,col] = gl_obj[col]\"\"\"\n",
    "        \n",
    "    print(\"%s memory usage before optimization: %s\"%(df.name,mem_usage(gl_obj)))\n",
    "    print(\"%s memory usage after optimization: %s\"%(df.name,mem_usage(converted_obj)))\n",
    "    print(\"      ***       \")\n",
    "          \n",
    "    return converted_obj    \n",
    "\n",
    "def mysql_db(sql):    \n",
    "    conn = mysql_db_conn()\n",
    "    df = pd.read_sql_query(sql,conn)\n",
    "    return df\n",
    "\n",
    "def mysql_db_large(db,tbl,columns,limit,chunks=10000):\n",
    "    \n",
    "    query = \"select \"+columns+ \" from \"+db+\".\"+tbl+ \" limit \"+limit+\";\"\n",
    "    splits = []\n",
    "    conn = mysql_db_conn()\n",
    "    for chunk in pd.read_sql(query,conn,chunksize=chunks):\n",
    "        splits.append(chunk)\n",
    "        concat_df  = pd.concat(splits,ignore_index=True)\n",
    "    return concat_df\n",
    "\n",
    "## Data preprocessing helper methods --\n",
    "\n",
    "def drop_allsame(df):\n",
    "    '''\n",
    "    Function to remove any columns which have same value all across\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    to_drop = list()\n",
    "    for i in df.columns:\n",
    "        if len(df.loc[:,i].unique()) == 1:\n",
    "            to_drop.append(i)\n",
    "    return df.drop(to_drop,axis =1)\n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean'):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(df.ix[:,i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(df.ix[:,i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(df.ix[:,i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(method ='ffill')\n",
    "    \n",
    "    elif type(how) == int or type(how) == float:\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how,i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(how)\n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df\n",
    "\n",
    "def treat_missing_categorical(df,columns,how = 'mode'):\n",
    "    '''\n",
    "    Function to treat missing values in categorical columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mode', any string or numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(df.ix[:,i].mode()[0])\n",
    "    elif type(how) == str:\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how,i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(how)\n",
    "    elif type(how) == int or type(how) == float:\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how,i))\n",
    "            df.ix[:,i] = df.ix[:,i].fillna(str(how))\n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df\n",
    "    \n",
    "def min_max_scaler(df,columns):\n",
    "    '''\n",
    "    Function to do Min-Max scaling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = df.columns\n",
    "    return data, scaler\n",
    "\n",
    "def label_encoder(df,columns):\n",
    "    '''\n",
    "    Function to label encode\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be label encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with lable encoded columns\n",
    "        - le_dict = Dictionary of all the column and their label encoders\n",
    "    '''\n",
    "    le_dict = {}\n",
    "    for c in columns:\n",
    "        print(\"Label encoding column - {0}\".format(c))\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df[c].values.astype('str')))\n",
    "        df[c] = lbl.transform(list(df[c].values.astype('str')))\n",
    "        #le_dict[c] = lbl\n",
    "        le_dict[c] = dict(zip(lbl.classes_, lbl.transform(lbl.classes_)))\n",
    "    return df, le_dict\n",
    "\n",
    "def one_hot_encoder(df, columns):\n",
    "    '''\n",
    "    Function to do one-hot encoded\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be one-hot encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with one-hot encoded columns\n",
    "    '''\n",
    "    for each in columns:\n",
    "        print(\"One-Hot encoding column - {0}\".format(each))\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df.drop(columns,axis = 1)\n",
    "\n",
    "## feature selection helper method\n",
    "\n",
    "def pecor_selector(X, y,n=10):\n",
    "    cor_list = []\n",
    "    \n",
    "    #Normalization: no\n",
    "    #Impute missing values: yes\n",
    "    \n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-n:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in X.columns]\n",
    "    #cor_support = [True if i in cor_feature]\n",
    "    print(str(len(cor_feature)), 'selected features')\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def kbest_selector(X, y, n=10):\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    #Normalization: MinMaxScaler (values should be bigger than 0)\n",
    "    #Impute missing values: yes\n",
    "    if n>len(X.columns):\n",
    "        n = len(X.columns)\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=n)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    print(str(len(chi_feature)), 'selected features')\n",
    "    return chi_support, chi_feature\n",
    "    \n",
    "def rfe_selector(X, y, n=18):    \n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "     \n",
    "    #Normalization: depend on the used model; yes for LR\n",
    "    #Impute missing values: depend on the used model; yes for LR\n",
    "    \n",
    "    rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=n, step=1, verbose=5)\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    rfe_selector.fit(X_norm, y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "    #rfe_feature = X.loc[:].columns.tolist()\n",
    "    print(str(len(rfe_feature)), 'selected features')\n",
    "    return rfe_support, rfe_feature\n",
    "    \n",
    "def rforest_selector(X, y, n=10):\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "    #Normalization: No\n",
    "    #Impute missing values: Yes\n",
    "    \n",
    "    if n>len(X.columns):\n",
    "        n = len(X.columns)    \n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=50), threshold=10)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    #embeded_rf_support = [True if i in embeded_rf_selector.get_support() else False for i in X.columns]\n",
    "    #embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "    embeded_rf_feature = X.iloc[:,embeded_rf_support.argsort()[::-1][:n]].columns.tolist()\n",
    "    embeded_rf_support = [True if i in embeded_rf_feature else False for i in X.columns]\n",
    "    print(str(len(embeded_rf_feature)), 'selected features')\n",
    "    return embeded_rf_support, embeded_rf_feature\n",
    "\n",
    "\n",
    "def select_features(X,y,n,rfe=0):    \n",
    "\tcor_support, cor_feature = pecor_selector(X,y,n)\n",
    "\tchi_support, chi_feature = kbest_selector(X,y,n)\n",
    "\tembeded_rf_support, embeded_rf_feature = rforest_selector(X,y,n)\n",
    "\n",
    "\tif rfe==1.0:\n",
    "\t\trfe_support, rfe_feature = rfe_selector(X, y,n)\n",
    "\t\tfeature_selection_df = pd.DataFrame({'Feature':X.columns, 'Chi-2':chi_support, 'RFE':rfe_support, 'Random Forest':embeded_rf_support})\n",
    "\t\t\n",
    "\n",
    "\t# put all selection together\n",
    "\tfeature_selection_df = pd.DataFrame({'Feature':X.columns, 'pearson':cor_support , 'Chi-2':chi_support, 'Random Forest':embeded_rf_support})\n",
    "\t# count the selected times for each feature\n",
    "\tfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "\t# display the top n\n",
    "\tfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "\tfeature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "\t#feature_selection_df.index = range(len(feature_selection_df))\n",
    "\treturn feature_selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Complete leads dataset & carry forward features --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    364608\n",
       "2017    274238\n",
       "2016    260695\n",
       "Name: createdDate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 906715 obs in sales matx\n",
      "There are 89712 obs where order are present in sales matx\n",
      "There are 817003 obs where order are NOT present in sales matx\n",
      "***************************\n",
      "These obs will be part of sample set for our classification model !\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdDate</th>\n",
       "      <th>Id_x</th>\n",
       "      <th>lead_id</th>\n",
       "      <th>phone</th>\n",
       "      <th>Device_Type__c</th>\n",
       "      <th>Company__c</th>\n",
       "      <th>Country__c</th>\n",
       "      <th>Company</th>\n",
       "      <th>Designation__c</th>\n",
       "      <th>Funded_By__c</th>\n",
       "      <th>Status</th>\n",
       "      <th>LeadSource</th>\n",
       "      <th>Lead_Creation_mode__c</th>\n",
       "      <th>Primary_Course_Interested__c</th>\n",
       "      <th>Primary_Category_of_Interest__c</th>\n",
       "      <th>Primary_Training_Type__c</th>\n",
       "      <th>Lead_Owner_Email__c</th>\n",
       "      <th>lead_entry_source</th>\n",
       "      <th>sitemodule</th>\n",
       "      <th>lead_creation_mode</th>\n",
       "      <th>lead_source</th>\n",
       "      <th>lead_medium</th>\n",
       "      <th>lead_campaign</th>\n",
       "      <th>lead_adgroup</th>\n",
       "      <th>lead_utm_term</th>\n",
       "      <th>leadQueryType</th>\n",
       "      <th>leadTrainingType</th>\n",
       "      <th>leadGeo</th>\n",
       "      <th>leadCity</th>\n",
       "      <th>leadCountry</th>\n",
       "      <th>querySourceString</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Id_y</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>StageName</th>\n",
       "      <th>Closed_Date__c</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>orderNumber</th>\n",
       "      <th>Opportunity__c</th>\n",
       "      <th>payment_date__c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-21 21:08:56</td>\n",
       "      <td>00Q0I00000rpjEVUAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07982385527</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>Online</td>\n",
       "      <td>Data Science with Python</td>\n",
       "      <td>Data Science &amp; Business Intelligence</td>\n",
       "      <td>Online Classroom Flexi-Pass</td>\n",
       "      <td>shikha.mishra@simplilearn.net</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>chat_triggered</td>\n",
       "      <td>online</td>\n",
       "      <td>google</td>\n",
       "      <td>organic</td>\n",
       "      <td>(organic)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not provided)</td>\n",
       "      <td>B2C</td>\n",
       "      <td>LVC</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>https://www.simplilearn.com/big-data-and-analy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-22 05:05:08</td>\n",
       "      <td>00Q0I00000rpjT1UAI</td>\n",
       "      <td>0060I00000XZw2BQAT</td>\n",
       "      <td>7126160</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Anse la Raye RC Infant School</td>\n",
       "      <td>Saint Lucia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Principal</td>\n",
       "      <td>Self</td>\n",
       "      <td>Converted</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>Online</td>\n",
       "      <td>Digital Marketing Specialist</td>\n",
       "      <td>Digital Marketing</td>\n",
       "      <td>Online Classroom Flexi-Pass</td>\n",
       "      <td>sajith.cs@simplilearn.net</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>course preview</td>\n",
       "      <td>online</td>\n",
       "      <td>google</td>\n",
       "      <td>cpc</td>\n",
       "      <td>search-brand-row</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simpli  learn</td>\n",
       "      <td>B2C</td>\n",
       "      <td>LVC</td>\n",
       "      <td>AMERICAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saint Lucia</td>\n",
       "      <td>https://www.simplilearn.com/digital-marketing/...</td>\n",
       "      <td>81270.0</td>\n",
       "      <td>0060I00000XZw2BQAT</td>\n",
       "      <td>0010I00001b8cd3QAA</td>\n",
       "      <td>Closed Won</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>30938.0</td>\n",
       "      <td>CSTM_67V0YZ53NRB</td>\n",
       "      <td>0060I00000XZw2BQAT</td>\n",
       "      <td>2018-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-22 05:10:19</td>\n",
       "      <td>00Q0I00000rpjTQUAY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>Online</td>\n",
       "      <td>Agile and Scrum Career Advancement Bundle</td>\n",
       "      <td>Agile and Scrum</td>\n",
       "      <td>Online Classroom Flexi-Pass</td>\n",
       "      <td>sarath@simplilearn.net</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>chat_triggered</td>\n",
       "      <td>online</td>\n",
       "      <td>(direct)</td>\n",
       "      <td>(none)</td>\n",
       "      <td>(direct)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2C</td>\n",
       "      <td>LVC</td>\n",
       "      <td>AMERICAS</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://www.simplilearn.com/agile-and-scrum/pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-22 07:10:18</td>\n",
       "      <td>00Q0I00000rpjaCUAQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9969593909</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>Online</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Science &amp; Business Intelligence</td>\n",
       "      <td>Online Classroom Flexi-Pass</td>\n",
       "      <td>mousumi.das@simplilearn.net</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>bundle agenda</td>\n",
       "      <td>online</td>\n",
       "      <td>accounts.simplilearn.com</td>\n",
       "      <td>referral</td>\n",
       "      <td>(referral)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2C</td>\n",
       "      <td>LVC</td>\n",
       "      <td>IN</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>India</td>\n",
       "      <td>https://www.simplilearn.com/big-data-and-analy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-22 11:05:07</td>\n",
       "      <td>00Q0I00000rpjkvUAA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09444202015</td>\n",
       "      <td>desktop</td>\n",
       "      <td>TCS</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>Self</td>\n",
       "      <td>Inactive</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>Online</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Science &amp; Business Intelligence</td>\n",
       "      <td>Online Classroom Flexi-Pass</td>\n",
       "      <td>sharath.bazar@simplilearn.net</td>\n",
       "      <td>SL-Website</td>\n",
       "      <td>bundle agenda</td>\n",
       "      <td>online</td>\n",
       "      <td>google</td>\n",
       "      <td>cpc</td>\n",
       "      <td>search-bigdata-ds-low-exact-in-adgroup-data-sc...</td>\n",
       "      <td>data science</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>B2C</td>\n",
       "      <td>LVC</td>\n",
       "      <td>IN</td>\n",
       "      <td>Chetput</td>\n",
       "      <td>India</td>\n",
       "      <td>https://www.simplilearn.com/big-data-and-analy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           createdDate                Id_x             lead_id        phone  \\\n",
       "0  2017-10-21 21:08:56  00Q0I00000rpjEVUAY                 NaN  07982385527   \n",
       "1  2017-10-22 05:05:08  00Q0I00000rpjT1UAI  0060I00000XZw2BQAT      7126160   \n",
       "2  2017-10-22 05:10:19  00Q0I00000rpjTQUAY                 NaN          NaN   \n",
       "3  2017-10-22 07:10:18  00Q0I00000rpjaCUAQ                 NaN   9969593909   \n",
       "4  2017-10-22 11:05:07  00Q0I00000rpjkvUAA                 NaN  09444202015   \n",
       "\n",
       "  Device_Type__c                     Company__c     Country__c Company  \\\n",
       "0        desktop                            NaN          India     NaN   \n",
       "1        desktop  Anse la Raye RC Infant School    Saint Lucia     NaN   \n",
       "2            NaN                            NaN  United States     NaN   \n",
       "3        desktop                            NaN          India     NaN   \n",
       "4        desktop                            TCS          India     NaN   \n",
       "\n",
       "  Designation__c Funded_By__c     Status  LeadSource Lead_Creation_mode__c  \\\n",
       "0            NaN          NaN   Inactive  SL-Website                Online   \n",
       "1      Principal         Self  Converted  SL-Website                Online   \n",
       "2            NaN          NaN   Inactive  SL-Website                Online   \n",
       "3            NaN          NaN   Inactive  SL-Website                Online   \n",
       "4              .         Self   Inactive  SL-Website                Online   \n",
       "\n",
       "                Primary_Course_Interested__c  \\\n",
       "0                   Data Science with Python   \n",
       "1               Digital Marketing Specialist   \n",
       "2  Agile and Scrum Career Advancement Bundle   \n",
       "3                             Data Scientist   \n",
       "4                             Data Scientist   \n",
       "\n",
       "        Primary_Category_of_Interest__c     Primary_Training_Type__c  \\\n",
       "0  Data Science & Business Intelligence  Online Classroom Flexi-Pass   \n",
       "1                     Digital Marketing  Online Classroom Flexi-Pass   \n",
       "2                       Agile and Scrum  Online Classroom Flexi-Pass   \n",
       "3  Data Science & Business Intelligence  Online Classroom Flexi-Pass   \n",
       "4  Data Science & Business Intelligence  Online Classroom Flexi-Pass   \n",
       "\n",
       "             Lead_Owner_Email__c lead_entry_source      sitemodule  \\\n",
       "0  shikha.mishra@simplilearn.net        SL-Website  chat_triggered   \n",
       "1      sajith.cs@simplilearn.net        SL-Website  course preview   \n",
       "2         sarath@simplilearn.net        SL-Website  chat_triggered   \n",
       "3    mousumi.das@simplilearn.net        SL-Website   bundle agenda   \n",
       "4  sharath.bazar@simplilearn.net        SL-Website   bundle agenda   \n",
       "\n",
       "  lead_creation_mode               lead_source lead_medium  \\\n",
       "0             online                    google     organic   \n",
       "1             online                    google         cpc   \n",
       "2             online                  (direct)      (none)   \n",
       "3             online  accounts.simplilearn.com    referral   \n",
       "4             online                    google         cpc   \n",
       "\n",
       "                                       lead_campaign  lead_adgroup  \\\n",
       "0                                          (organic)           NaN   \n",
       "1                                   search-brand-row           NaN   \n",
       "2                                           (direct)           NaN   \n",
       "3                                         (referral)           NaN   \n",
       "4  search-bigdata-ds-low-exact-in-adgroup-data-sc...  data science   \n",
       "\n",
       "    lead_utm_term leadQueryType leadTrainingType   leadGeo leadCity  \\\n",
       "0  (not provided)           B2C              LVC        IN      NaN   \n",
       "1   simpli  learn           B2C              LVC  AMERICAS      NaN   \n",
       "2             NaN           B2C              LVC  AMERICAS  Atlanta   \n",
       "3             NaN           B2C              LVC        IN   Mumbai   \n",
       "4  data scientist           B2C              LVC        IN  Chetput   \n",
       "\n",
       "     leadCountry                                  querySourceString  \\\n",
       "0          India  https://www.simplilearn.com/big-data-and-analy...   \n",
       "1    Saint Lucia  https://www.simplilearn.com/digital-marketing/...   \n",
       "2  United States  https://www.simplilearn.com/agile-and-scrum/pr...   \n",
       "3          India  https://www.simplilearn.com/big-data-and-analy...   \n",
       "4          India  https://www.simplilearn.com/big-data-and-analy...   \n",
       "\n",
       "   Unnamed: 0_x                Id_y           AccountId   StageName  \\\n",
       "0           NaN                 NaN                 NaN         NaN   \n",
       "1       81270.0  0060I00000XZw2BQAT  0010I00001b8cd3QAA  Closed Won   \n",
       "2           NaN                 NaN                 NaN         NaN   \n",
       "3           NaN                 NaN                 NaN         NaN   \n",
       "4           NaN                 NaN                 NaN         NaN   \n",
       "\n",
       "  Closed_Date__c  Unnamed: 0_y       orderNumber      Opportunity__c  \\\n",
       "0            NaN           NaN               NaN                 NaN   \n",
       "1     2018-09-20       30938.0  CSTM_67V0YZ53NRB  0060I00000XZw2BQAT   \n",
       "2            NaN           NaN               NaN                 NaN   \n",
       "3            NaN           NaN               NaN                 NaN   \n",
       "4            NaN           NaN               NaN                 NaN   \n",
       "\n",
       "  payment_date__c  \n",
       "0             NaT  \n",
       "1      2018-09-21  \n",
       "2             NaT  \n",
       "3             NaT  \n",
       "4             NaT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leads_feat_all = ['createdDate', 'Id','lead_id',\n",
    "       'phone', 'Device_Type__c',  'Company__c',\n",
    "       'Company', 'Designation__c',\n",
    "       'Funded_By__c', 'Status', 'Site_Module__c',\n",
    "       'LeadSource', 'Lead_Creation_mode__c',\n",
    "       'Primary_Course_Interested__c', 'Primary_Category_of_Interest__c',\n",
    "       'Primary_Training_Type__c', 'City__c', 'Country__c', 'Country',\n",
    "       'Entry_Page__c', 'Lead_Owner_Email__c',\n",
    "       'Lead Campaign',\n",
    "       'sitemodule', 'lead_creation_mode', 'lead_entry_source', 'lead_source',\n",
    "       'lead_medium', 'lead_campaign', 'lead_adgroup', 'lead_utm_term',\n",
    "       'leadQueryType', 'leadTrainingType', 'leadGeo',\n",
    "       'leadCity', 'leadCountry', 'querySourceString']\n",
    "\n",
    "leads_feat_raw_only = ['createdDate', 'Id','lead_id',\n",
    "       'phone', 'Device_Type__c',  'Company__c',\n",
    "       'Company', 'Designation__c',\n",
    "       'Funded_By__c', 'Status', 'Site_Module__c',\n",
    "       'LeadSource', 'Lead_Creation_mode__c',\n",
    "       'Primary_Course_Interested__c', 'Primary_Category_of_Interest__c',\n",
    "       'Primary_Training_Type__c', 'City__c', 'Country__c', 'Country',\n",
    "       'Entry_Page__c', 'Lead_Owner_Email__c']\n",
    "  \n",
    "leads_feat_enterprise_only = ['Lead Campaign','Id','lead_id',\n",
    "       'sitemodule', 'lead_creation_mode', 'lead_entry_source', 'lead_source',\n",
    "       'lead_medium', 'lead_campaign', 'lead_adgroup', 'lead_utm_term',\n",
    "       'leadQueryType', 'leadTrainingType', 'leadGeo',\n",
    "       'leadCity', 'leadCountry', 'querySourceString']\n",
    "\n",
    "leads_feat_working01 = ['createdDate', 'Id','lead_id',\n",
    "       'phone', 'Device_Type__c',  'Company__c','Country__c',\n",
    "       'Company', 'Designation__c',\n",
    "       'Funded_By__c', 'Status',\n",
    "       'LeadSource', 'Lead_Creation_mode__c',\n",
    "       'Primary_Course_Interested__c', 'Primary_Category_of_Interest__c',\n",
    "       'Primary_Training_Type__c',\n",
    "       'Lead_Owner_Email__c','lead_entry_source',\n",
    "       'sitemodule', 'lead_creation_mode', 'lead_source',\n",
    "       'lead_medium', 'lead_campaign', 'lead_adgroup', 'lead_utm_term',\n",
    "       'leadQueryType', 'leadTrainingType', 'leadGeo',\n",
    "       'leadCity', 'leadCountry', 'querySourceString']\n",
    "\n",
    "leads_raw = pd.read_csv('C:/Users/abhranshu/Desktop/Incubating/Leads Scoring/leads_master_query_dump25nov18.csv',low_memory=True)\n",
    "leads_raw = leads_raw[leads_feat_working01]\n",
    "# keep only data from 2017 onwards\n",
    "leads_raw = leads_raw.loc[pd.DatetimeIndex(leads_raw.createdDate).year>2015]\n",
    "#pd.DatetimeIndex(leads_raw.createdDate).year.value_counts()\n",
    "\n",
    "sf_opp_raw = pd.read_csv('C:/Users/abhranshu/Desktop/Incubating/Leads Scoring/opportunity_b_dump.csv')\n",
    "\n",
    "sf_orders_raw = pd.read_csv('C:/Users/abhranshu/Desktop/Incubating/Leads Scoring/orders_b_dump.csv')\n",
    "sf_orders_raw.payment_date__c = pd.to_datetime(sf_orders_raw.payment_date__c)\n",
    "\n",
    "display(pd.DatetimeIndex(leads_raw.createdDate).year.value_counts())\n",
    "\n",
    "leads_opp_concat = pd.merge(leads_raw,sf_opp_raw,how='left',left_on='lead_id',right_on='Id')\n",
    "leads_opp_ord_concat = pd.merge(leads_opp_concat,sf_orders_raw,how='left',left_on='lead_id',right_on='Opportunity__c')\n",
    "\n",
    "print(\"There are total %r obs in sales matx\" %(len(leads_opp_ord_concat)))\n",
    "print(\"There are %r obs where order are present in sales matx\" %(len(leads_opp_ord_concat[~leads_opp_ord_concat.orderNumber.isnull()])))\n",
    "print(\"There are %r obs where order are NOT present in sales matx\" %(len(leads_opp_ord_concat[leads_opp_ord_concat.orderNumber.isnull()])))\n",
    "print(\"***************************\")\n",
    "print(\"These obs will be part of sample set for our classification model !\")\n",
    "\n",
    "leads_opp_ord_concat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Define Business Rules/ Feature Engineering --\n",
    "    - feature created: converted(boolean) for target variable of leads with/wo orders. \n",
    "    - feature created: countries_cat for top-leads & low-leads countries.\n",
    "    - feature created: sitemodule_cat for PSM & NON-PSM Site Modules.\n",
    "    - feature created: days_since_payment for elapsed payment date for leads.\n",
    "    - feature created: channel for leads channels.\n",
    "    - feature created: month_created_date for lead created date month.\n",
    "    - feature created: sitemodule_cluster for CPCABA+Chat leads.\n",
    "    - removing features which are low value addition to model (manual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass features from here:-\n",
    "pre_eng_features = [ 'Status','Primary_Course_Interested__c', 'Country__c', 'sitemodule',\n",
    "   'lead_campaign','lead_source','lead_entry_source','lead_medium','createdDate','lead_creation_mode',\n",
    "   'leadQueryType',  'leadTrainingType', 'leadGeo',\n",
    "   'payment_date__c','converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhranshu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "created_date_conv                 115437\n",
       "createdDate                       115437\n",
       "lead_campaign                       7041\n",
       "payment_date__c                      985\n",
       "days_since_payment                   985\n",
       "lead_source                          701\n",
       "Primary_Course_Interested__c         645\n",
       "lead_medium                          297\n",
       "Country__c                           209\n",
       "channelleadGeo_comb                  143\n",
       "sitemodule                            93\n",
       "lead_entry_source                     83\n",
       "channel                               57\n",
       "sitemodule_clusterleadGeo_comb        14\n",
       "month_created_date                    12\n",
       "Top_Channel                           11\n",
       "Top_psm_country_lead_gen              11\n",
       "Status                                 9\n",
       "leadGeo                                6\n",
       "leadTrainingType                       3\n",
       "sitemodule_cluster                     3\n",
       "lead_creation_mode                     2\n",
       "countries_cat                          2\n",
       "sitemodule_psm                         2\n",
       "top_psm_country                        2\n",
       "top_converted_sm                       2\n",
       "top_channel_flag                       2\n",
       "converted                              2\n",
       "leadQueryType                          1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set All Business Rules Here :- DUPLICATE !!!\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)\n",
    "\n",
    "############### Write code below ##################\n",
    "\n",
    "leads_opp_concat = pd.merge(leads_raw,sf_opp_raw,how='left',left_on='lead_id',right_on='Id')\n",
    "leads_opp_ord_concat = pd.merge(leads_opp_concat,sf_orders_raw,how='left',left_on='lead_id',right_on='Opportunity__c')\n",
    "\n",
    "leads_wo_orders_subset = leads_opp_ord_concat.loc[leads_opp_ord_concat.payment_date__c.isnull()][:70700]\n",
    "leads_w_orders_subset = leads_opp_ord_concat.loc[~leads_opp_ord_concat.payment_date__c.isnull()][:61477]\n",
    "leads_opp_ord_concat = pd.concat([leads_wo_orders_subset,leads_w_orders_subset])\n",
    "\n",
    "# Set target variable as Converted=1/0\n",
    "leads_opp_ord_concat.loc[~leads_opp_ord_concat.orderNumber.isnull(),'converted'] = 1\n",
    "leads_opp_ord_concat.loc[leads_opp_ord_concat.orderNumber.isnull(),'converted'] = 0\n",
    "\n",
    "\n",
    "target_var = 'converted'\n",
    "sales_matx = leads_opp_ord_concat[pre_eng_features]\n",
    "sales_matx.drop([],axis=1,inplace=True)\n",
    "\n",
    "## top-leads & low-leads countries\n",
    "country_list = sales_matx.Country__c.value_counts().sort_values(ascending=False).index\n",
    "top_leads_countries = country_list[0:10]\n",
    "low_leads_countries = country_list[10:]\n",
    "len(top_leads_countries)\n",
    "\n",
    "sales_matx.loc[sales_matx.Country__c.isin(top_leads_countries),'countries_cat']= 'top_leads_country'\n",
    "sales_matx.loc[sales_matx.Country__c.isin(low_leads_countries),'countries_cat']= 'low_leads_country'\n",
    "\n",
    "sales_matx['countries_cat'].value_counts()\n",
    "\n",
    "## PSM & NON-PSM Site Modules\n",
    "psm_list =['query box','toll','phone','request a call','na-payment','course agenda','bundle agenda','b2c masters','category_query_box','free_trial','free trial cancelled','previewobj','course preview','chat','chat_initiated','chat_triggered']\n",
    "\n",
    "sales_matx.loc[sales_matx.sitemodule.isin(psm_list),'sitemodule_psm']= 1\n",
    "sales_matx.loc[~sales_matx.sitemodule.isin(psm_list),'sitemodule_psm']= 0\n",
    "\n",
    "\n",
    "## Age of lead from Payment Date\n",
    "from datetime import datetime\n",
    "def compare_dates(date):\n",
    "    date_format = '%Y/%m/%d'\n",
    "    #current_date = datetime.strptime(date, date_format)\n",
    "    current_date = pd.to_datetime(date)\n",
    "    today = datetime.today()\n",
    "    diff = today - current_date\n",
    "    return diff.days\n",
    "\n",
    "#apply this function to your pandas dataframe to get days elapsed difference\n",
    "sales_matx['days_since_payment'] = sales_matx['payment_date__c'].apply(compare_dates)\n",
    "\n",
    "#sales_matx['days_since_leadclosed'] = sales_matx['Closed_Date__c'].apply(compare_dates)\n",
    "\n",
    "# Creating channel variable\n",
    "sales_matx.loc[sales_matx['lead_creation_mode'] == 'online', 'channel']= \"online-others\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"direct|(direct)\",na=False), 'channel']= \"direct\"\n",
    "sales_matx.loc[sales_matx['sitemodule'].str.contains(\"tollfree\",na=False), 'channel']= \"tollfree\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"steelhouse\",na=False), 'channel']= \"Steelhouse\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"facebook\",na=False)) & (sales_matx['lead_medium'].str.contains(\"rtg|dpa\",na=False)), 'channel']= \"facebook-rtg\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"quora-cpc\",na=False), 'channel']= \"quora-cpc\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"quora-referral\",na=False), 'channel']= \"quora-referral\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"quora-rtg\",na=False), 'channel']= \"quora-rtg\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"quora-rtg\",na=False), 'channel']= \"quora-rtg\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"google\",na=False)) & (sales_matx['lead_medium'].str.contains(\"cpc\",na=False))& (sales_matx['lead_campaign'].str.contains(\"youtube\",na=False)), 'channel']= \"youtube-cpc\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"youtube\",na=False)) & (sales_matx['lead_medium'].str.contains(\"cpc\",na=False)), 'channel']= \"youtube-cpc\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"google\",na=False)) & (sales_matx['lead_campaign'].str.contains(\"display\",na=False)), 'channel']= \"google-cpc-display\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"google\",na=False)) & (sales_matx['lead_campaign'].str.contains(\"search-brand\",na=False)), 'channel']= \"google-cpc-brand\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"utmgclid|google\",na=False))& (sales_matx['lead_medium'].str.contains(\"cpc\",na=False)) & (sales_matx['lead_campaign'].str.contains(\"atp\",na=False)), 'channel']= \"google-cpc-partner\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"utmgclid|google\",na=False)) & (sales_matx['lead_medium'].str.contains(\"cpc\",na=False)), 'channel']= \"google-cpc\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"bing\",na=False)) & (sales_matx['lead_campaign'].str.contains(\"brand\",na=False)), 'channel']= \"bing-brand\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"monster|naukri|dice|jobsahead|bayt|shine|iimjobs|indeed|hirist|leadgen\",na=False), 'channel']= 'lead-gen'\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"criteo\",na=False), 'channel']= \"criteo\"\n",
    "sales_matx.loc[sales_matx['lead_medium'].str.contains(\"organic\",na=False), 'channel']= \"organic\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"Drip\",na=False)) & (sales_matx['lead_medium'].str.contains(\"mcc|ebook|actonc\",na=False)), 'channel']= \"drip-cold\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"Drip\",na=False), 'channel']= \"drip\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"contentmarketing\",na=False)) | (sales_matx['lead_medium'].str.contains(\"contentmarketing\",na=False)) | (sales_matx['lead_campaign'].str.contains(\"contentmarketing\",na=False)), 'channel']= \"content-marketing\"\n",
    "sales_matx.loc[sales_matx['lead_medium'].str.contains(\"referral\",na=False), 'channel']= \"referral\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"linkedin\",na=False)) & (sales_matx['lead_medium'].str.contains(\"b2b|b2b-visit|b2b-leadgen\",na=False)), 'channel']= \"linkedin-b2b\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"linkedin\",na=False)) & (sales_matx['lead_medium'].str.contains(\"atp-leadgen|partner-visit|partner-leadgen\",na=False)), 'channel']= \"linkedin-partner\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"linkedin\",na=False)) & (sales_matx['lead_medium'].str.contains(\"leadgen|linkedin-cpc|b2c-visit|b2c-leadgen\",na=False)), 'channel']= \"linkedin-b2c\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"shareasale|payoom|komli|cj|omg|dgm|affiliate-cpa\",na=False)) | (sales_matx['lead_medium'].str.contains(\"affiliate-cpa\",na=False)), 'channel']= \"affiliate-networks\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"adroll\",na=False), 'channel']= \"adroll\"\n",
    "sales_matx.loc[~(sales_matx['lead_medium'].str.contains(\"social\",na=False)) & ((sales_matx['lead_source'].str.contains(\"facebook|facebook_leadgen\",na=False)) & (sales_matx['lead_campaign'].str.contains(\"marketing_facebook\",na=False))), 'channel']= \"facebook-b2c\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"facebook\",na=False)) & ((sales_matx['lead_medium'].str.contains(\"b2c-visit|b2c-leadgen\",na=False))), 'channel']= \"facebook-b2c\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"social\",na=False), 'channel']= \"social\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"affiliate\",na=False)) & ((sales_matx['lead_medium'].str.contains(\"affiliate-cpl|affiliate-cpm\",na=False))), 'channel']= \"media-buying\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"bizo\",na=False), 'channel']= \"bizo\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"adelement\",na=False), 'channel']= \"adelement\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"twitter\",na=False), 'channel']= \"twitter\"\n",
    "sales_matx.loc[sales_matx['lead_source'].str.contains(\"youtube\",na=False), 'channel']= \"youtube\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"mail\",na=False)) | (sales_matx['lead_medium'].str.contains(\"mail\",na=False)), 'channel']= \"sales-email\"\n",
    "sales_matx.loc[(sales_matx['lead_source'].str.contains(\"bing\",na=False)) & (sales_matx['lead_medium'].str.contains(\"cpc\",na=False)), 'channel']= \"sales-email\"\n",
    "sales_matx.loc[sales_matx['lead_medium'].str.contains(\"social\",na=False), 'channel']= \"social\"\n",
    "sales_matx.loc[sales_matx['lead_creation_mode'].str.contains(\"offline\",na=False), 'channel']= sales_matx['lead_entry_source']\n",
    "sales_matx.loc[sales_matx['channel'].isnull(),'channel'] = \"Other\"\n",
    "#sales_matx.channel.value_counts()\n",
    "\n",
    "\n",
    "# extracting month from created date\n",
    "sales_matx['created_date_conv'] = pd.to_datetime(sales_matx['createdDate'], format = '%Y-%m-%d %H:%M:%S')\n",
    "sales_matx['month_created_date'] = sales_matx.created_date_conv.dt.month\n",
    "\n",
    "# creating site module cluster\n",
    "sales_matx.loc[sales_matx['sitemodule'].str.contains(\"course agenda|course preview\",na=False), 'sitemodule_cluster']= 'Course Preview/ Agenda'\n",
    "sales_matx.loc[sales_matx['sitemodule'].str.contains(\"chat|call\",na=False), 'sitemodule_cluster']= 'chat/call'\n",
    "sales_matx.loc[sales_matx['sitemodule_cluster'].isnull(),'sitemodule_cluster'] = \"Other\"\n",
    "\n",
    "\n",
    "\"\"\"pat1 = r'PMP'\n",
    "pat2 = r'ITIL'\n",
    "pat3 = r'PRINCE2'\n",
    "pat4 = r'Scrum'\n",
    "pat5 = r'Big%Data|Hadoop|Spark'\n",
    "pat6 = r'AWS'\n",
    "pat7 = r'Azure'\n",
    "pat8 = r'Marketing'\n",
    "pat9 = r'Data%Science|Machine%learning|ML|Science|AI|Artificial|R%Programming'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat1),'product_name'] = 'PMP'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat2),'product_name'] = 'ITIL'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat3),'product_name'] = 'PRINCE2'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat4),'product_name'] = 'CSM'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat5),'product_name'] = 'Hadoop'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat6),'product_name'] = 'AWS-Cloud'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat7),'product_name'] = 'Azure-Cloud'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat8),'product_name'] = 'Digital-Marketing'\n",
    "sales_matx.loc[sales_matx.product_name__c.str.contains(pat9),'product_name'] = 'AWS-Cloud'\n",
    "sales_matx[:5]\"\"\"\n",
    "\n",
    "## Top_10_countries_based_on_leads_generating_PSM\n",
    "var_10_psm_country = sales_matx.groupby(['Country__c','sitemodule_psm']).size().to_frame('count').reset_index().sort_values(['count'],ascending=False)\n",
    "fet_top_psm_countries = var_10_psm_country[var_10_psm_country.sitemodule_psm==1]['Country__c'][:10]\n",
    "fet_botm_psm_countries = var_10_psm_country[var_10_psm_country.sitemodule_psm==1]['Country__c'][10:]\n",
    "top_psm_countries_vals = list(fet_top_psm_countries.values)\n",
    "sales_matx.loc[sales_matx.Country__c.isin(top_psm_countries_vals),'top_psm_country'] = 1\n",
    "sales_matx.loc[~sales_matx.Country__c.isin(top_psm_countries_vals),'top_psm_country'] = 0\n",
    "\n",
    "## Top_10_sitemodules_based_on_orders\n",
    "var_leads_w_ords=sales_matx[~sales_matx.payment_date__c.isnull()]\n",
    "var_top_converted_sm = var_leads_w_ords.groupby(['sitemodule']).size().to_frame('count').reset_index().sort_values(['count'],ascending=False)\n",
    "fet_top_converted_sm = var_top_converted_sm['sitemodule'][:10]\n",
    "fet_bottm_converted_sm = var_top_converted_sm['sitemodule'][10:]\n",
    "top_converted_sm_vals = list(fet_top_converted_sm.values)\n",
    "bottm_converted_sm_vals = list(fet_bottm_converted_sm.values)\n",
    "sales_matx.loc[sales_matx.sitemodule.isin(top_converted_sm_vals),'top_converted_sm'] = 1\n",
    "sales_matx.loc[sales_matx.sitemodule.isin(bottm_converted_sm_vals),'top_converted_sm'] = 0\n",
    "\n",
    "# top 10 channel based on lead gen\n",
    "var_10_channel = sales_matx.groupby(['channel']).size().to_frame('count').reset_index().sort_values(['count'],ascending=False)\n",
    "fet_top_channel = var_10_channel['channel'][:10]\n",
    "top_channel_vals = list(fet_top_channel.values)\n",
    "sales_matx.loc[sales_matx.channel.isin(top_channel_vals),'top_channel_flag'] = 1\n",
    "sales_matx.loc[~sales_matx.channel.isin(top_channel_vals),'top_channel_flag'] = 0\n",
    "\n",
    "def t(row):\n",
    "    if row['top_channel_flag'] == 1:\n",
    "        val = row['channel']\n",
    "    else:\n",
    "        val = 'Other'\n",
    "    return val\n",
    "sales_matx['Top_Channel'] = sales_matx.apply(t, axis=1)\n",
    "\n",
    "sales_matx = sales_matx.loc[sales_matx.leadQueryType=='B2C']\n",
    "\n",
    "## Top_10_countries_based_on_leads_generating_PSM - variable\n",
    "\n",
    "var_10_psm_country = sales_matx.groupby(['Country__c','sitemodule_psm']).size().to_frame('count').reset_index().sort_values(['count'],ascending=False)\n",
    "fet_top_psm_countries = var_10_psm_country[var_10_psm_country.sitemodule_psm==1]['Country__c'][:10]\n",
    "fet_botm_psm_countries = var_10_psm_country[var_10_psm_country.sitemodule_psm==1]['Country__c'][10:]\n",
    "top_psm_countries_vals = list(fet_top_psm_countries.values)\n",
    "sales_matx.loc[sales_matx.Country__c.isin(top_psm_countries_vals),'top_psm_country'] = 1\n",
    "sales_matx.loc[~sales_matx.Country__c.isin(top_psm_countries_vals),'top_psm_country'] = 0\n",
    "\n",
    "#sales_matx.top_psm_country.unique()\n",
    "def f(row):\n",
    "    if row['top_psm_country'] == 1:\n",
    "        val = row['Country__c']\n",
    "    else:\n",
    "        val = 'Other'\n",
    "    return val\n",
    "sales_matx['Top_psm_country_lead_gen'] = sales_matx.apply(f, axis=1)\n",
    "\n",
    "\n",
    "def combine_features(df,fet1,fet2):\n",
    "    prefx = df[fet1].name+df[fet2].name\n",
    "    sufx = '_comb'\n",
    "    for item1 in df[fet1].unique():\n",
    "        for item2 in df[fet2].unique():\n",
    "            df.loc[(df[fet1]==item1) & (df[fet2]==item2),(prefx+sufx)]=item1+'-'+str(item2)\n",
    "\n",
    "combine_features(sales_matx,'sitemodule_cluster','leadGeo')\n",
    "combine_features(sales_matx,'channel','leadGeo')\n",
    "\n",
    "display(sales_matx.apply(pd.Series.nunique).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_date_conv                 115437\n",
       "createdDate                       115437\n",
       "lead_campaign                       7041\n",
       "payment_date__c                      985\n",
       "days_since_payment                   985\n",
       "lead_source                          701\n",
       "Primary_Course_Interested__c         645\n",
       "lead_medium                          297\n",
       "Country__c                           209\n",
       "channelleadGeo_comb                  143\n",
       "sitemodule                            93\n",
       "lead_entry_source                     83\n",
       "channel                               57\n",
       "sitemodule_clusterleadGeo_comb        14\n",
       "month_created_date                    12\n",
       "Top_Channel                           11\n",
       "Top_psm_country_lead_gen              11\n",
       "Status                                 9\n",
       "leadGeo                                6\n",
       "leadTrainingType                       3\n",
       "sitemodule_cluster                     3\n",
       "lead_creation_mode                     2\n",
       "countries_cat                          2\n",
       "sitemodule_psm                         2\n",
       "top_psm_country                        2\n",
       "top_converted_sm                       2\n",
       "top_channel_flag                       2\n",
       "converted                              2\n",
       "leadQueryType                          1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop redundant/less_imp fields\n",
    "\n",
    "sales_matx_copy = sales_matx.copy()\n",
    "sales_matx_temp = sales_matx.copy()\n",
    "\n",
    "def drop_features(df,lst):\n",
    "    return df.drop(lst,axis=1,inplace=True)\n",
    "\n",
    "def diff(first, second):\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]\n",
    "\n",
    "def col_selector(df,keep_cols,use=False):\n",
    "    if use==True:\n",
    "        mandate_cols = []        \n",
    "      \n",
    "        rem_cols = diff(df.columns,keep_cols)\n",
    "        rem_cols.remove('converted')\n",
    "        \n",
    "        drop_features(df,rem_cols)\n",
    "    else: \n",
    "        print(\"you have not opted for column selector functionality!!\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def filter_method_fselection(df,how_many=5,sel_criteria='cutoff',use=False):\n",
    "    if use==True:\n",
    "        print(\"Called after feature engineering !!\")\n",
    "        print(\"If column selector is used, this method is called after that !!\")\n",
    "        print(\"some methods like pearson, rf require encoded data ??\")\n",
    "        X_lab_enc,X_lab_enc_dict = label_encoder(df,df.columns)\n",
    "        feature_selection_df = select_features(X_lab_enc.loc[:,X_lab_enc.columns!='converted'],X_lab_enc['converted'],how_many)\n",
    "        if sel_criteria == 'max':\n",
    "            f_methd_list = feature_selection_df.loc[feature_selection_df['Total']==max(feature_selection_df['Total'])]['Feature'].tolist()\n",
    "        elif sel_criteria == 'cutoff':\n",
    "            f_methd_list = feature_selection_df.loc[feature_selection_df['Total']>1.0]['Feature'].tolist()\n",
    "    return f_methd_list\n",
    "    \n",
    "\"\"\"\n",
    "rem_feats = ['Primary_Course_Interested__c',  'top_channel_flag','lead_campaign','lead_source','lead_medium','createdDate',\n",
    "             'created_date_conv','days_since_payment','payment_date__c','month_created_date','top_psm_country','top_converted_sm','lead_creation_mode'\n",
    "             ,'sitemodule_psm','channelleadGeo_comb','sitemodule_clusterleadGeo_comb','Top_psm_country_lead_gen','Top_Channel','Status'\n",
    "             ,'countries_cat','leadQueryType']\n",
    "\n",
    "drop_feats(sales_matx_copy,rem_feats)\n",
    "post_eng_selected_features = list(sales_matx_copy.columns)\"\"\"\n",
    "\n",
    "display(sales_matx_copy.apply(pd.Series.nunique).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called after feature engineering !!\n",
      "If column selector is used, this method is called after that !!\n",
      "some methods like pearson, rf require encoded data ??\n",
      "Label encoding column - Country__c\n",
      "Label encoding column - sitemodule\n",
      "Label encoding column - leadGeo\n",
      "Label encoding column - converted\n",
      "Label encoding column - channel\n",
      "3 selected features\n",
      "3 selected features\n",
      "3 selected features\n",
      "Filling missing values with mode for columns - sitemodule\n",
      "Filling missing values with mode for columns - leadGeo\n",
      "Filling missing values with mode for columns - Country__c\n",
      "One-Hot encoding column - sitemodule\n",
      "One-Hot encoding column - leadGeo\n",
      "One-Hot encoding column - Country__c\n"
     ]
    }
   ],
   "source": [
    "# imputation of missing values\n",
    "\n",
    "col_selector(sales_matx_copy,['Country__c', 'sitemodule', 'leadGeo','channel'],True)\n",
    "col_selector(sales_matx_temp,['Country__c', 'sitemodule', 'leadGeo','channel'],True)\n",
    "\n",
    "fets_selected_f_filter_methd = filter_method_fselection(sales_matx_temp,3,'cutoff',True)\n",
    "rem_cols = diff(sales_matx_copy.columns,fets_selected_f_filter_methd)\n",
    "rem_cols.remove('converted')\n",
    "drop_features(sales_matx_copy,rem_cols)\n",
    "\n",
    "sales_matx_imputed = treat_missing_categorical(sales_matx_copy,fets_selected_f_filter_methd,how = 'mode')\n",
    "\n",
    "\"\"\"only_OHE_these = [\n",
    " 'leadQueryType',\n",
    " 'leadTrainingType',\n",
    " 'leadGeo',\n",
    " 'Top_psm_country_lead_gen',\n",
    " 'Top_Channel',\n",
    " 'Status',\n",
    " 'countries_cat',\n",
    " 'channel',\n",
    " 'sitemodule_cluster',\n",
    " 'sitemodule_clusterleadGeo_comb',\n",
    " 'channelleadGeo_comb']\n",
    " \n",
    " only_OHE_these = [\n",
    " 'channel',\n",
    " 'sitemodule',\n",
    " 'leadTrainingType',\n",
    " 'leadGeo']\n",
    "\"\"\"\n",
    "\n",
    "only_OHE_these = fets_selected_f_filter_methd\n",
    "\n",
    "# OHE- one hot enconding of features\n",
    "#fet = post_eng_selected_features.copy()\n",
    "#fet.remove('converted')\n",
    "OHE_enc = one_hot_encoder(sales_matx_imputed,only_OHE_these)\n",
    "X_enc = OHE_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- create training and validation set -- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Fit the enconded dataset to a Random Forest Model --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n",
      "Thresh=0.000, n=308, Accuracy: 68.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7fdae66cf87f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mselection_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mselection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# eval model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mselect_X_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use feature importance for feature selection\n",
    "\n",
    "from numpy import sort\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# load data\n",
    "\n",
    "X, y = X_enc.loc[:,X_enc.columns!='converted'], X_enc[target_var]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "# fit model on all training data\n",
    "#model = XGBClassifier()\n",
    "model = GradientBoostingClassifier(random_state=0, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "\t# select features using threshold\n",
    "\tselection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "\tselect_X_train = selection.transform(X_train)\n",
    "\t# train model\n",
    "\tselection_model = GradientBoostingClassifier()\n",
    "\tselection_model.fit(select_X_train, y_train)\n",
    "\t# eval model\n",
    "\tselect_X_test = selection.transform(X_test)\n",
    "\ty_pred = selection_model.predict(select_X_test)\n",
    "\tpredictions = [round(value) for value in y_pred]\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ascending' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-28eaacbf5f68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'ascending' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "sorted(thresholds[thresholds>0.0],ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X, y = X_enc.loc[:,X_enc.columns!='converted'], X_enc[target_var]\n",
    "X_train_dtm, X_test_dtm, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "print(color.BOLD + color.BLUE + \"train dataset shape: %s , test dataset shape:%s \"%(X_train_dtm.shape,X_test_dtm.shape))\n",
    "\n",
    "#regr2 = SelectFromModel(RandomForestClassifier(n_estimators=n,max_features=10), threshold='1.25*median')\n",
    "#rf = RandomForestClassifier(max_depth=4,n_estimators=80, random_state=0,criterion='entropy')\n",
    "#rf = RandomForestClassifier(max_depth=15,n_estimators=200, random_state=0,max_features=7,min_samples_split=2)\n",
    "rf = GradientBoostingClassifier(random_state=0, max_depth=4)\n",
    "\n",
    "rf.fit(X_train_dtm,y_train)\n",
    "y_pred_class = rf.predict(X_test_dtm)\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"*****************************************\")\n",
    "print(color.BOLD + color.BLUE + \"Accuracy obtained from model is :%.2f%% \"%(metrics.accuracy_score(y_test, y_pred_class)*100))\n",
    "print(\"*****************************************\")\n",
    "print(\"*****************************************\")\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "#features= selected_features\n",
    "fet_cols = X_enc.loc[:,X_enc.columns!='converted'].columns\n",
    "features= fet_cols\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:15]] \n",
    "print(color.BOLD + color.BLUE + \"         ******           \")\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "print (classification_report(y_test, rf.predict(X_test_dtm)))\n",
    "#fpr, tpr, thresholds = roc_curve(y_test, regr2.predict_proba(X_test_dtm), pos_label=3)\n",
    "print(color.BOLD + color.BLUE + \"         ******           \")\n",
    "\n",
    "#f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=fet_cols)\n",
    "feat_importances.nlargest(10).plot(kind='bar')\n",
    "\n",
    "test_score_matx = X_test_dtm.copy()\n",
    "test_score_matx['score'] = np.round(rf.predict_proba(test_score_matx)[:,1]*100,2)\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"*****************************************\")\n",
    "test_score_matx = X_test_dtm.copy()\n",
    "test_score_matx['score'] = np.round(rf.predict_proba(test_score_matx)[:,1]*100,2)\n",
    "print(\"Maximum predicted score  is: %r\"%(max(test_score_matx['score'])))\n",
    "print(\"*****************************************\")\n",
    "print(\"*****************************************\")\n",
    "\n",
    "# info of the highest score lead\n",
    "result = test_score_matx.loc[test_score_matx['score']==max(test_score_matx['score'])]\n",
    "k = [i[0] for i in feature_importances[:15]]\n",
    "k.append('score')\n",
    "debug_result_df = result[k]\n",
    "result.loc[result.index[:1]][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_score_matx.loc[test_score_matx['score']>80.0]\n",
    "k = [i[0] for i in feature_importances[:15]]\n",
    "k.append('score')\n",
    "sales_matx.loc[sales_matx.index.isin(result.index)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=sales_matx_copy['leadGeo'], columns=sales_matx_copy['converted'],normalize='all',\n",
    "                        margins=True, margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(sales_matx_copy.loc[(sales_matx_copy.converted==1.0) & (sales_matx_copy.sitemodule=='Converted')])/len(sales_matx_copy)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colo in sales_matx_copy.columns:\n",
    "    if ((colo!='converted') & (len(sales_matx[colo].unique())<10)):\n",
    "        f, ax = plt.subplots(figsize=(4,4))\n",
    "        sns.heatmap(pd.crosstab(index=sales_matx_copy[colo], columns=sales_matx_copy['converted'],normalize='all',\n",
    "                        margins=True, margins_name=\"Total\"),cmap=\"YlGnBu\", annot=True, cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='LVC')])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='LVC') & (sales_matx.converted==1.0)])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='CR')])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='CR') & (sales_matx.converted==1.0)])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='OSL')])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.leadTrainingType=='OSL') & (sales_matx.converted==1.0)])/len(sales_matx)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(sales_matx.loc[(sales_matx.channel=='offline') & (sales_matx.leadGeo=='AMERICAS')])/len(sales_matx)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(sales_matx.loc[(sales_matx.converted==1.0) & (sales_matx.sitemodule_cluster=='Course Preview/ Agenda')])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.converted==1.0) & (sales_matx.sitemodule_cluster=='chat/call')])/len(sales_matx)*100)\n",
    "display(len(sales_matx.loc[(sales_matx.converted==1.0) & (sales_matx.sitemodule_cluster=='Other')])/len(sales_matx)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** plotting the cumulative importance line ** \n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)-98))\n",
    "\n",
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances[:10]]\n",
    "sorted_features = [importance[0] for importance in feature_importances[:10]]\n",
    "\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_score_matx['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = test_score_matx.loc[test_score_matx['score']==max(test_score_matx['score'])]\n",
    "result = test_score_matx.loc[test_score_matx['score']==74.68]\n",
    "k = [i[0] for i in feature_importances[:15]]\n",
    "k.append('score')\n",
    "sales_matx.loc[sales_matx.index.isin(result.index) & (sales_matx.converted==0)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[result.index[:1]][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "orig = np.array([6, 9, 8, 2, 5, 4, 5, 3, 3, 6])\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "encoded = ohe.fit_transform(orig.reshape(-1, 1)) # input needs to be column-wise\n",
    "\n",
    "decoded = encoded.dot(ohe.active_features_).astype(int)\n",
    "#print(np.allclose(orig, decoded))\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "instances = X_test_dtm.loc[X_test_dtm.index==632699]\n",
    "\n",
    "print (\"Instance 0 prediction:\", rf.predict(instances[:1]))\n",
    "#print (\"Instance 1 prediction:\", rf.predict(instances[1:2]))\n",
    "\n",
    "prediction, bias, contributions = ti.predict(rf, instances)\n",
    "\n",
    "for i in range(len(instances)):\n",
    "    print (\"Instance\", i)\n",
    "    print (\"Bias (trainset mean)\", bias[i])\n",
    "    print (\"Feature contributions:\")\n",
    "    for c, feature in zip(contributions[i],X_enc.columns):\n",
    "        print (feature, c)\n",
    "    print (\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_matx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "            \n",
    "cs = ColumnSelector(columns=[\"Status\"])\n",
    "cs.fit_transform(sales_matx).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.select_dtypes(include=[self.dtype])\n",
    "    \n",
    "preprocess_pipeline = make_pipeline(\n",
    "    ColumnSelector(columns=x_cols),\n",
    "    FeatureUnion(transformer_list=[\n",
    "        (\"numeric_features\", make_pipeline(\n",
    "            TypeSelector(np.number),\n",
    "            Imputer(strategy=\"median\"),\n",
    "            StandardScaler()\n",
    "        )),\n",
    "        (\"categorical_features\", make_pipeline(\n",
    "            TypeSelector(\"category\"),\n",
    "            Imputer(strategy=\"most_frequent\"),\n",
    "            OneHotEncoder()\n",
    "        )),\n",
    "        (\"boolean_features\", make_pipeline(\n",
    "            TypeSelector(\"bool\"),\n",
    "            Imputer(strategy=\"most_frequent\")\n",
    "        ))\n",
    "    ])\n",
    ")    \n",
    "    \n",
    "ts = TypeSelector(\"category\")\n",
    "ts.fit_transform(X).head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import pmlb\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (12, 8)})\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "\n",
    "\n",
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.select_dtypes(include=[self.dtype])\n",
    "    \n",
    "class Onehotenc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for each in X.columns:\n",
    "            print(\"One-Hot encoding column - {0}\".format(each))\n",
    "            dummies = pd.get_dummies(X[each], prefix=each, drop_first=False)\n",
    "            X = pd.concat([X, dummies], axis=1)\n",
    "        return X.drop(columns,axis = 1)\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X    \n",
    "    \n",
    "#df = pmlb.fetch_data('churn', return_X_y=False)\n",
    "df = sales_matx\n",
    "\n",
    "# Remove the target column and the phone number\n",
    "x_cols = [c for c in df if c not in [\"converted\"]]\n",
    "\n",
    "binary_features = ['sitemodule_psm','top_psm_country','top_converted_sm','top_channel_flag']\n",
    "\n",
    "catg = []\n",
    "for c in sales_matx.select_dtypes(include=['object']):\n",
    "    catg.append(c)   \n",
    "categorical_features = catg\n",
    "\n",
    "# Column types are defaulted to floats\n",
    "X = df\n",
    "#(\n",
    "    #df\n",
    "    #.drop([\"converted\"], axis=1)\n",
    "    #.astype(float)\n",
    "#)\n",
    "X[binary_features] = X[binary_features].astype(\"bool\")\n",
    "\n",
    "# Categorical features can't be set all at once\n",
    "for f in categorical_features:\n",
    "    X[f] = X[f].astype(\"category\")\n",
    "\n",
    "y = df.converted\n",
    "\n",
    "# Partition data set into training/test split (2 to 1 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "    ColumnSelector(columns=x_cols),\n",
    "    FeatureUnion(transformer_list=[\n",
    "        (\"numeric_features\", make_pipeline(\n",
    "            TypeSelector(np.number),\n",
    "            Imputer(strategy=\"median\"),\n",
    "            StandardScaler()\n",
    "        )),\n",
    "        (\"categorical_features\", make_pipeline(\n",
    "            TypeSelector(\"category\"),\n",
    "            Imputer(strategy=\"most_frequent\"),\n",
    "            OneHotEncoder()\n",
    "        )),\n",
    "        (\"boolean_features\", make_pipeline(\n",
    "            TypeSelector(\"bool\"),\n",
    "            Imputer(strategy=\"most_frequent\")\n",
    "        ))\n",
    "    ])\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [        \n",
    "            (\"ohe\",Onehotenc(\"category\")),\n",
    "            (\"debug\", Debug()),\n",
    "            (\"nmf\", NMF())\n",
    "        \n",
    "    ])\n",
    "\n",
    "classifier_pipeline = make_pipeline(\n",
    "    preprocess_pipeline,\n",
    "    SVC(kernel=\"rbf\", random_state=42)\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"svc__gamma\": [0.1 * x for x in range(1, 2)]\n",
    "}\n",
    "\n",
    "classifier_model = GridSearchCV(classifier_pipeline, param_grid, cv=3)\n",
    "#classifier_model.fit(X_train, y_train)\n",
    "X_train[:3]\n",
    "pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_score_gen(df,fet_imp=\"\"):\n",
    "\n",
    "    var_lead = 1\n",
    "    var_sm = 1\n",
    "    var_p_age = 1\n",
    "    var_cp = 1\n",
    "    var_ctype = 1\n",
    "        \n",
    "    if (df['Region'] == 'IN'):\n",
    "        var_lead = var_lead*50\n",
    "    elif df['Region'] == \"AMERICAS\":\n",
    "        var_lead = var_lead*30\n",
    "    elif df['Region'] == \"ROW\":\n",
    "        var_lead = var_lead*20\n",
    "    \n",
    "    if df['sitemodule_type'] == \"PSM\":\n",
    "        var_sm = var_sm*50\n",
    "    elif df['sitemodule_type'] == \"NON-PSM\":\n",
    "        var_sm = var_sm*20\n",
    "        \n",
    "    if df['days_since_payment'] <= 273:\n",
    "        var_p_age = var_p_age*50\n",
    "    elif df['days_since_payment'] > 273:\n",
    "        var_p_age = var_p_age*20 \n",
    "        \n",
    "    if df['CourseProgress'] >= 75.0:\n",
    "        var_cp = var_cp*50\n",
    "    elif df['CourseProgress'] < 75.0:\n",
    "        var_cp = var_cp*20  \n",
    "        \n",
    "    if df['course_type'] == \"high_selling\":\n",
    "        var_ctype = var_ctype*50\n",
    "    elif df['course_type'] == \"low_selling\":\n",
    "        var_ctype = var_ctype*20     \n",
    "        \n",
    "    #return var_lead*fet_imp['leadGeo']+var_sm*fet_imp['Site_Module__c']\n",
    "    return var_p_age*0.65+var_ctype*0.28+var_sm*0.05+var_lead*0.02\n",
    "    \n",
    "combined_working_set['lead_score'] = combined_working_set.apply(df_score_gen, axis = 1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
